{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca7e89a8",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ§  Sentiment Analysis for Eâ€‘commerce Product Reviews\n",
    "\n",
    "This notebook implements an **endâ€‘toâ€‘end solution** to automatically assess and categorize customer sentiments (**negative**, **neutral**, **positive**) from product reviews.\n",
    "\n",
    "**What you'll get here:**\n",
    "- Clean, reproducible pipeline (EDA â†’ preprocessing â†’ baselines â†’ Transformer fineâ€‘tuning â†’ evaluation â†’ error analysis).\n",
    "- Ready-to-run code for both **classic ML** (TFâ€‘IDF + Logistic/SVM) and **Transformer** (**DistilBERT**) models.\n",
    "- Reusable **inference utilities** and an **optional FastAPI microservice** for deployment.\n",
    "\n",
    "> Expected columns in your dataset CSV:\n",
    "> - `Product ID` (string or int)\n",
    "> - `Product Review` (text)\n",
    "> - `Sentiment` (one of: `negative`, `neutral`, `positive`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a439aa98",
   "metadata": {},
   "source": [
    "## 1) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1957df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you're on Colab or a fresh environment, uncomment the installs below.\n",
    "# You can skip any line if the package already exists in your environment.\n",
    "\n",
    "# %pip install -U pip\n",
    "# %pip install pandas numpy scikit-learn matplotlib\n",
    "# %pip install transformers==4.44.2 datasets==2.21.0 accelerate==0.34.2 evaluate==0.4.2\n",
    "# %pip install torch --index-url https://download.pytorch.org/whl/cpu   # or proper CUDA wheel for your GPU\n",
    "# %pip install fastapi uvicorn\n",
    "# %pip install wordcloud emoji textblob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaf3fa8",
   "metadata": {},
   "source": [
    "## 2) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55545d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding,\n",
    "    TrainingArguments, Trainer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521573e7",
   "metadata": {},
   "source": [
    "## 3) Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073caa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ” EDIT: set the path to your CSV (with columns: Product ID, Product Review, Sentiment).\n",
    "DATA_PATH = \"reviews.csv\"  # e.g., \"data/ecomm_reviews.csv\"\n",
    "\n",
    "assert Path(DATA_PATH).exists(), f\"CSV not found at {DATA_PATH}. Please update DATA_PATH.\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Normalize expected column names (case-insensitive / extra spaces)\n",
    "rename_map = {c: c.strip() for c in df.columns}\n",
    "df.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "# Try to auto-detect reasonable column names if variations exist\n",
    "def find_col(candidates):\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    for name in candidates:\n",
    "        for col in df.columns:\n",
    "            if col.lower().strip() == name.lower().strip():\n",
    "                return col\n",
    "    raise KeyError(f\"Couldn't find any of: {candidates} in columns: {list(df.columns)}\")\n",
    "\n",
    "COL_ID = find_col([\"Product ID\", \"product_id\", \"productid\", \"id\"])\n",
    "COL_TEXT = find_col([\"Product Review\", \"review\", \"text\", \"comment\", \"content\"])\n",
    "COL_LABEL = find_col([\"Sentiment\", \"label\", \"sentiment_label\", \"target\"])\n",
    "\n",
    "df = df[[COL_ID, COL_TEXT, COL_LABEL]].copy()\n",
    "df.columns = [\"product_id\", \"text\", \"label\"]\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3b81a7",
   "metadata": {},
   "source": [
    "## 4) Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019d35a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic stats\n",
    "display(df.sample(min(5, len(df)), random_state=42))\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Nulls:\\n\", df.isnull().sum())\n",
    "\n",
    "# Drop rows with missing text or label\n",
    "df = df.dropna(subset=[\"text\", \"label\"]).copy()\n",
    "df[\"text\"] = df[\"text\"].astype(str).str.strip()\n",
    "df = df[df[\"text\"].str.len() > 0]\n",
    "\n",
    "# Class balance\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df[\"label\"].value_counts())\n",
    "\n",
    "# Text length distribution\n",
    "df[\"char_len\"] = df[\"text\"].str.len()\n",
    "df[\"word_len\"] = df[\"text\"].str.split().str.len()\n",
    "\n",
    "print(\"\\nLength stats (chars):\")\n",
    "print(df[\"char_len\"].describe())\n",
    "\n",
    "print(\"\\nLength stats (words):\")\n",
    "print(df[\"word_len\"].describe())\n",
    "\n",
    "# Plot length histogram (optional)\n",
    "plt.figure()\n",
    "df[\"word_len\"].hist(bins=40)\n",
    "plt.title(\"Word count distribution\")\n",
    "plt.xlabel(\"Words per review\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb0464c",
   "metadata": {},
   "source": [
    "## 5) Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c45a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import emoji\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"http\\S+|www\\.\\S+\", \" \", s)          # URLs\n",
    "    s = re.sub(r\"@\\w+|#\\w+\", \" \", s)                 # mentions/hashtags\n",
    "    s = emoji.replace_emoji(s, replace=\" \")          # remove/space emojis\n",
    "    s = re.sub(r\"[^a-z0-9\\s\\.\\,\\!\\?]\", \" \", s)      # keep basic punctuations\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "df[\"text_clean\"] = df[\"text\"].map(clean_text)\n",
    "\n",
    "# Map labels to ids (negative=0, neutral=1, positive=2)\n",
    "label_set = sorted(df[\"label\"].str.lower().unique().tolist())\n",
    "label2id = {\"negative\":0, \"neutral\":1, \"positive\":2}\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "\n",
    "# Validate labels\n",
    "invalid = set(label_set) - set(label2id.keys())\n",
    "if invalid:\n",
    "    raise ValueError(f\"Unexpected labels in data: {invalid}. Expected one of {list(label2id.keys())}\")\n",
    "\n",
    "df[\"y\"] = df[\"label\"].str.lower().map(label2id)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331f5014",
   "metadata": {},
   "source": [
    "## 6) Train / Validation / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebfac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"y\"])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df[\"y\"])\n",
    "\n",
    "for name, d in [(\"train\", train_df), (\"val\", val_df), (\"test\", test_df)]:\n",
    "    print(name, d.shape, Counter(d[\"y\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d433fbbb",
   "metadata": {},
   "source": [
    "## 7) Classic ML Baselines (TFâ€‘IDF + LR / SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3b7d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, y_train = train_df[\"text_clean\"], train_df[\"y\"]\n",
    "X_val, y_val = val_df[\"text_clean\"], val_df[\"y\"]\n",
    "X_test, y_test = test_df[\"text_clean\"], test_df[\"y\"]\n",
    "\n",
    "# Pipeline: TFIDF + Logistic Regression\n",
    "pipe_lr = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(ngram_range=(1,2), min_df=2, max_df=0.9)),\n",
    "    (\"clf\", LogisticRegression(max_iter=200, n_jobs=None))\n",
    "])\n",
    "\n",
    "# Simple grid for LR\n",
    "param_grid_lr = {\n",
    "    \"clf__C\": [0.5, 1.0, 2.0],\n",
    "    \"clf__penalty\": [\"l2\"],\n",
    "    \"clf__solver\": [\"lbfgs\"]\n",
    "}\n",
    "grid_lr = GridSearchCV(pipe_lr, param_grid_lr, cv=3, scoring=\"f1_macro\", n_jobs=-1, verbose=1)\n",
    "grid_lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best LR params:\", grid_lr.best_params_)\n",
    "val_pred_lr = grid_lr.predict(X_val)\n",
    "print(\"\\nValidation report (LR):\\n\", classification_report(y_val, val_pred_lr, target_names=[id2label[i] for i in [0,1,2]]))\n",
    "\n",
    "# Pipeline: TFIDF + Linear SVM\n",
    "pipe_svm = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(ngram_range=(1,2), min_df=2, max_df=0.9)),\n",
    "    (\"clf\", LinearSVC())\n",
    "])\n",
    "# Minimal grid for SVM\n",
    "param_grid_svm = {\"clf__C\": [0.5, 1.0, 2.0]}\n",
    "grid_svm = GridSearchCV(pipe_svm, param_grid_svm, cv=3, scoring=\"f1_macro\", n_jobs=-1, verbose=1)\n",
    "grid_svm.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best SVM params:\", grid_svm.best_params_)\n",
    "val_pred_svm = grid_svm.predict(X_val)\n",
    "print(\"\\nValidation report (SVM):\\n\", classification_report(y_val, val_pred_svm, target_names=[id2label[i] for i in [0,1,2]]))\n",
    "\n",
    "# Pick the better baseline on val macro-F1\n",
    "val_f1_lr = f1_score(y_val, val_pred_lr, average=\"macro\")\n",
    "val_f1_svm = f1_score(y_val, val_pred_svm, average=\"macro\")\n",
    "best_baseline, best_model = (\"LR\", grid_lr) if val_f1_lr >= val_f1_svm else (\"SVM\", grid_svm)\n",
    "print(f\"\\nBest baseline: {best_baseline} (macro-F1={max(val_f1_lr, val_f1_svm):.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4847cf",
   "metadata": {},
   "source": [
    "## 8) Transformer Fineâ€‘tuning (DistilBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bae63bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df[[\"text\", \"y\"]].rename(columns={\"y\":\"labels\"}))\n",
    "val_ds   = Dataset.from_pandas(val_df[[\"text\", \"y\"]].rename(columns={\"y\":\"labels\"}))\n",
    "test_ds  = Dataset.from_pandas(test_df[[\"text\", \"y\"]].rename(columns={\"y\":\"labels\"}))\n",
    "\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True)\n",
    "val_ds   = val_ds.map(tokenize_fn, batched=True)\n",
    "test_ds  = test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "id2label_map = {0:\"negative\", 1:\"neutral\", 2:\"positive\"}\n",
    "label2id_map = {\"negative\":0, \"neutral\":1, \"positive\":2}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,\n",
    "    id2label=id2label_map,\n",
    "    label2id=label2id_map\n",
    ")\n",
    "\n",
    "metric_f1 = evaluate.load(\"f1\")\n",
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": metric_acc.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"f1_macro\": metric_f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "        \"f1_weighted\": metric_f1.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"artifacts/distilbert-sentiment\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train();\n",
    "\n",
    "# Evaluate on validation and test\n",
    "val_metrics = trainer.evaluate(val_ds)\n",
    "test_metrics = trainer.evaluate(test_ds)\n",
    "print(\"Validation metrics:\", val_metrics)\n",
    "print(\"Test metrics:\", test_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d26d3b9",
   "metadata": {},
   "source": [
    "## 9) Evaluation & Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d5ea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(name, y_true, y_pred):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(classification_report(y_true, y_pred, target_names=[id2label[i] for i in [0,1,2]]))\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1,2])\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"f1_weighted\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "    }\n",
    "\n",
    "# Baseline on test\n",
    "test_pred_bl = best_model.predict(test_df[\"text_clean\"])\n",
    "baseline_scores = evaluate_model(f\"Baseline ({best_baseline})\", test_df[\"y\"], test_pred_bl)\n",
    "\n",
    "# Transformer on test\n",
    "tf_logits = trainer.predict(test_ds).predictions\n",
    "tf_pred = tf_logits.argmax(axis=-1)\n",
    "transformer_scores = evaluate_model(\"DistilBERT\", test_df[\"y\"], tf_pred)\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(\"Baseline:\", baseline_scores)\n",
    "print(\"DistilBERT:\", transformer_scores)\n",
    "\n",
    "# Show a few most confusing examples for the transformer\n",
    "mis_idx = np.where(tf_pred != test_df[\"y\"].values)[0].tolist()\n",
    "print(f\"\\nMisclassified examples: {len(mis_idx)}\")\n",
    "for i in mis_idx[:10]:\n",
    "    print(\"---\")\n",
    "    print(\"Text:\", test_df.iloc[i][\"text\"][:500])\n",
    "    print(\"True:\", id2label[test_df.iloc[i][\"y\"]], \"Pred:\", id2label[tf_pred[i]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef5e42b",
   "metadata": {},
   "source": [
    "## 10) Inference Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a8a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_best_transformer():\n",
    "    # Reload the best model saved by Trainer\n",
    "    m = AutoModelForSequenceClassification.from_pretrained(\"artifacts/distilbert-sentiment/checkpoint-best\", local_files_only=False)\n",
    "    tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    return m, tok\n",
    "\n",
    "def predict_sentiment(texts, model=None, tok=None, batch_size=32):\n",
    "    if model is None or tok is None:\n",
    "        model, tok = load_best_transformer()\n",
    "    enc = tok(texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits\n",
    "    pred_ids = logits.argmax(dim=-1).tolist()\n",
    "    return [id2label[i] for i in pred_ids]\n",
    "\n",
    "# Example:\n",
    "# predict_sentiment([\"Loved it! Battery lasts long.\", \"Terrible quality, stopped working in a week.\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d15e4f",
   "metadata": {},
   "source": [
    "## 11) Save Baseline Model & Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6307c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import joblib\n",
    "Path(\"artifacts/baseline\").mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(best_model, \"artifacts/baseline/tfidf_baseline.joblib\")\n",
    "print(\"Saved baseline to artifacts/baseline/tfidf_baseline.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f80fc5",
   "metadata": {},
   "source": [
    "## 12) Optional: FastAPI Microservice (for deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a5612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FASTAPI_CODE = r'''\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "id2label = {0:\"negative\", 1:\"neutral\", 2:\"positive\"}\n",
    "\n",
    "class Payload(BaseModel):\n",
    "    texts: List[str]\n",
    "\n",
    "app = FastAPI(title=\"Sentiment API\", version=\"1.0.0\")\n",
    "\n",
    "MODEL_DIR = \"artifacts/distilbert-sentiment/checkpoint-best\"\n",
    "TOKENIZER = \"distilbert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "tok = AutoTokenizer.from_pretrained(TOKENIZER)\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(payload: Payload):\n",
    "    enc = tok(payload.texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits\n",
    "    preds = logits.argmax(dim=-1).tolist()\n",
    "    labels = [id2label[i] for i in preds]\n",
    "    return {\"predictions\": labels}\n",
    "'''\n",
    "\n",
    "Path(\"artifacts/api\").mkdir(parents=True, exist_ok=True)\n",
    "with open(\"artifacts/api/app.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(FASTAPI_CODE)\n",
    "\n",
    "print(\"FastAPI app written to artifacts/api/app.py\")\n",
    "print(\"Run locally with:\")\n",
    "print(\"  uvicorn artifacts.api.app:app --reload --port 8000\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f2db9a",
   "metadata": {},
   "source": [
    "\n",
    "## 13) Notes & Tips\n",
    "- Start with the **baseline** model and verify you have stable metrics, then fineâ€‘tune **DistilBERT**.\n",
    "- Use **macroâ€‘F1** to trade off class imbalance. Consider up/downâ€‘sampling if classes are skewed.\n",
    "- For small datasets, add **data augmentation** (e.g., backâ€‘translation, synonym replacement).\n",
    "- Keep an eye on **leakage** (no duplicates across splits).\n",
    "- Log experiments and **seed** randomness for reproducibility.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
